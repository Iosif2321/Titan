================================================================================
TRANSFORMERFUSION ARCHITECTURE
Sprint 21 - 2025-12-30
================================================================================

OVERVIEW
--------
TransformerFusion uses self-attention to learn optimal model combinations with
comprehensive overfitting prevention through dropout, L2 regularization, early
stopping, LR scheduling, gradient clipping, and train/validation monitoring.


NETWORK ARCHITECTURE
--------------------

Input: 4 Models × (prob_up, prob_down)
│
├─ TrendVIC:      [0.65, 0.35]
├─ Oscillator:    [0.55, 0.45]
├─ VolumeMetrix:  [0.60, 0.40]
└─ ML_Classifier: [0.58, 0.42]
│
│  Shape: (batch=1, n_models=4, features=2)
│
▼
┌────────────────────────────────────────────────────────────────────────┐
│  INPUT PROJECTION                                                       │
│  Linear(2 → hidden_dim=32)                                             │
│                                                                         │
│  [0.65, 0.35] → [0.12, -0.34, 0.56, ..., 0.23]  (32 dims)            │
│  [0.55, 0.45] → [0.08, -0.12, 0.34, ..., 0.45]                        │
│  [0.60, 0.40] → [0.15, -0.28, 0.41, ..., 0.31]                        │
│  [0.58, 0.42] → [0.11, -0.19, 0.38, ..., 0.37]                        │
│                                                                         │
│  Shape: (1, 4, 32)                                                     │
└────────────────────────────────────────────────────────────────────────┘
│
▼
┌────────────────────────────────────────────────────────────────────────┐
│  TRANSFORMER FUSION LAYER                                              │
│                                                                         │
│  ┌──────────────────────────────────────────────────────────────────┐ │
│  │  MULTI-HEAD SELF-ATTENTION (num_heads=2)                         │ │
│  │                                                                   │ │
│  │  For each model i, compute attention to all models j:            │ │
│  │                                                                   │ │
│  │  Q = W_q × X  (Query:  "What am I looking for?")                │ │
│  │  K = W_k × X  (Key:    "What info do I have?")                  │ │
│  │  V = W_v × X  (Value:  "The actual info")                       │ │
│  │                                                                   │ │
│  │  Attention(Q,K,V) = softmax(QK^T / √d) × V                      │ │
│  │                                                                   │ │
│  │  Example attention matrix (model i → model j):                   │ │
│  │                                                                   │ │
│  │         To: TrendVIC  Osc  VolM  ML                              │ │
│  │  From:                                                           │ │
│  │  TrendVIC    0.40    0.20  0.25  0.15  ← Trusts itself most     │ │
│  │  Osc         0.15    0.45  0.20  0.20  ← Trusts itself most     │ │
│  │  VolM        0.20    0.15  0.50  0.15  ← Trusts itself most     │ │
│  │  ML          0.25    0.20  0.20  0.35  ← Balanced attention     │ │
│  │                                                                   │ │
│  │  Output: Weighted combination of all models' representations     │ │
│  │  Shape: (1, 4, 32)                                               │ │
│  └──────────────────────────────────────────────────────────────────┘ │
│  │                                                                     │
│  ├─> Dropout(0.2) → Prevent overfitting                               │
│  ├─> Residual Connection: X_new = X_old + Attention(X_old)            │
│  └─> Layer Normalization                                              │
│                                                                         │
│  ┌──────────────────────────────────────────────────────────────────┐ │
│  │  FEED-FORWARD NETWORK                                            │ │
│  │                                                                   │ │
│  │  FFN(x) = ReLU(W_1 × x + b_1) × W_2 + b_2                       │ │
│  │                                                                   │ │
│  │  Layer 1: Linear(32 → 128)                                       │ │
│  │  Activation: ReLU                                                │ │
│  │  Dropout(0.2)                                                    │ │
│  │  Layer 2: Linear(128 → 32)                                       │ │
│  │  Dropout(0.2)                                                    │ │
│  │                                                                   │ │
│  │  Purpose: Add non-linearity and model complexity                 │ │
│  └──────────────────────────────────────────────────────────────────┘ │
│  │                                                                     │
│  ├─> Residual Connection: X_new = X_old + FFN(X_old)                  │
│  └─> Layer Normalization                                              │
│                                                                         │
│  Final shape: (1, 4, 32)                                               │
└────────────────────────────────────────────────────────────────────────┘
│
▼
┌────────────────────────────────────────────────────────────────────────┐
│  GLOBAL POOLING                                                        │
│                                                                         │
│  Average over all 4 models:                                            │
│  pooled = mean(all_model_representations, dim=1)                       │
│                                                                         │
│  Shape: (1, 4, 32) → (1, 32)                                          │
└────────────────────────────────────────────────────────────────────────┘
│
▼
┌────────────────────────────────────────────────────────────────────────┐
│  OUTPUT PROJECTION                                                     │
│                                                                         │
│  Layer 1: Linear(32 → 16)                                             │
│  Activation: ReLU                                                      │
│  Dropout(0.2)                                                          │
│  Layer 2: Linear(16 → 2)  [logits for UP/DOWN]                        │
│                                                                         │
│  Shape: (1, 32) → (1, 2)                                              │
└────────────────────────────────────────────────────────────────────────┘
│
▼
┌────────────────────────────────────────────────────────────────────────┐
│  SOFTMAX                                                               │
│                                                                         │
│  Logits: [0.45, -0.35]                                                │
│  Softmax: exp(x_i) / sum(exp(x_j))                                    │
│                                                                         │
│  Output probabilities:                                                 │
│    prob_up   = 0.62                                                    │
│    prob_down = 0.38                                                    │
│                                                                         │
│  Shape: (1, 2)                                                         │
└────────────────────────────────────────────────────────────────────────┘
│
▼
Final Output: (prob_up=0.62, prob_down=0.38)


TRAINING FLOW
-------------

┌─────────────────────────────────────────────────────────────────────┐
│  INCOMING PREDICTION                                                 │
│                                                                      │
│  Inputs:  model_outputs = [TrendVIC, Osc, VolM, ML]                │
│           features = {close: 50000, rsi: 55, ...}                   │
│           actual_direction = "UP"                                   │
└─────────────────────────────────────────────────────────────────────┘
│
├─> Random Split (80/20)
│
├─ 80% → ┌─────────────────────────────────────┐
│        │  TRAINING BUFFER                     │
│        │  Max size: 1000 samples (FIFO)      │
│        │  Current: [sample_1, ..., sample_N] │
│        └─────────────────────────────────────┘
│
└─ 20% → ┌─────────────────────────────────────┐
         │  VALIDATION BUFFER                   │
         │  Max size: 200 samples (FIFO)       │
         │  Current: [sample_1, ..., sample_M] │
         └─────────────────────────────────────┘

When training buffer ≥ min_samples (200):
│
▼
┌─────────────────────────────────────────────────────────────────────┐
│  TRAINING STEP                                                       │
│                                                                      │
│  1. Sample mini-batch (size=32) from training buffer                │
│  2. Forward pass through network                                    │
│  3. Compute loss: KL_divergence(predictions, targets)               │
│  4. Backward pass (compute gradients)                               │
│  5. Gradient clipping (max norm = 1.0)                              │
│  6. Optimizer step (AdamW with L2 regularization)                   │
│  7. LR scheduler step (warmup + cosine decay)                       │
│  8. Record train_loss                                                │
└─────────────────────────────────────────────────────────────────────┘
│
▼
┌─────────────────────────────────────────────────────────────────────┐
│  VALIDATION STEP (every 10 validation samples)                      │
│                                                                      │
│  1. Forward pass on ALL validation samples (no training)            │
│  2. Compute val_loss                                                 │
│  3. Compare to best_val_loss:                                       │
│     - If improved: reset patience_counter                           │
│     - If not: increment patience_counter                            │
│  4. Check for overfitting:                                          │
│     - If val_loss > train_loss × 1.2 AND patience ≥ 50:            │
│       → Set is_overfitting = True                                   │
└─────────────────────────────────────────────────────────────────────┘


OVERFITTING PREVENTION MECHANISMS
----------------------------------

1. DROPOUT (rate=0.2)
   ┌────────────────────────────────────────────────────┐
   │  During Training:                                  │
   │  - Randomly zero 20% of neurons                    │
   │  - Forces redundancy                               │
   │  - Prevents co-adaptation                          │
   │                                                    │
   │  During Inference:                                 │
   │  - All neurons active                              │
   │  - Outputs scaled by (1 - dropout_rate)           │
   └────────────────────────────────────────────────────┘

2. L2 REGULARIZATION (lambda=0.01)
   ┌────────────────────────────────────────────────────┐
   │  Loss = Data_Loss + λ × Σ(weights²)               │
   │                                                    │
   │  Effect:                                           │
   │  - Penalizes large weights                         │
   │  - Encourages simpler models                       │
   │  - Implemented via AdamW optimizer                 │
   └────────────────────────────────────────────────────┘

3. EARLY STOPPING
   ┌────────────────────────────────────────────────────┐
   │  Monitor: best_val_loss                           │
   │  Patience: 50 steps                               │
   │  Delta: 0.001 (min improvement)                   │
   │                                                    │
   │  If val_loss doesn't improve for 50 steps:        │
   │  → Set is_overfitting = True                      │
   │  → Warning to user                                 │
   │  (In production: would restore best checkpoint)    │
   └────────────────────────────────────────────────────┘

4. LEARNING RATE SCHEDULING
   ┌────────────────────────────────────────────────────┐
   │  Phase 1: WARMUP (0 → 100 steps)                  │
   │  LR: 0.0 → 0.001 (linear)                         │
   │                                                    │
   │  Phase 2: DECAY (100+ steps)                      │
   │  LR: 0.001 → 0.0005 (cosine)                      │
   │                                                    │
   │  Benefit:                                          │
   │  - Stable initial training                         │
   │  - Fine-tuning at end                             │
   └────────────────────────────────────────────────────┘

5. GRADIENT CLIPPING (max_norm=1.0)
   ┌────────────────────────────────────────────────────┐
   │  If ||gradients|| > 1.0:                          │
   │    gradients = gradients / ||gradients||          │
   │                                                    │
   │  Prevents:                                         │
   │  - Exploding gradients                             │
   │  - Unstable training                               │
   │  - NaN values                                      │
   └────────────────────────────────────────────────────┘

6. TRAIN/VALIDATION SPLIT (80/20)
   ┌────────────────────────────────────────────────────┐
   │  Training Set:   Used for gradient updates         │
   │  Validation Set: Used for performance monitoring   │
   │                                                    │
   │  Key: Validation set NEVER used for training       │
   │  → Unbiased estimate of generalization            │
   └────────────────────────────────────────────────────┘

7. OVERFITTING DETECTION
   ┌────────────────────────────────────────────────────┐
   │  Monitor ratio: val_loss / train_loss             │
   │                                                    │
   │  Healthy:     ratio ≈ 1.0                         │
   │  Warning:     ratio > 1.2                         │
   │  Overfitting: ratio > 1.5                         │
   │                                                    │
   │  Action: Increase regularization or reduce        │
   │          model complexity                          │
   └────────────────────────────────────────────────────┘


ATTENTION MECHANISM VISUALIZATION
----------------------------------

Example: How model 0 (TrendVIC) attends to other models

Step 1: Compute Queries, Keys, Values
┌──────────────────────────────────────────────────────┐
│  TrendVIC:     Q=[0.5, 0.2, ...]  K=[0.1, 0.3, ...] │
│  Oscillator:   Q=[0.3, 0.4, ...]  K=[0.2, 0.1, ...] │
│  VolumeMetrix: Q=[0.4, 0.3, ...]  K=[0.3, 0.2, ...] │
│  ML:           Q=[0.2, 0.5, ...]  K=[0.4, 0.1, ...] │
└──────────────────────────────────────────────────────┘

Step 2: Compute Attention Scores (Q × K^T)
┌──────────────────────────────────────────────────────┐
│  TrendVIC query × all keys:                          │
│    vs TrendVIC:     0.5×0.1 + 0.2×0.3 + ... = 0.8   │
│    vs Oscillator:   0.5×0.2 + 0.2×0.1 + ... = 0.3   │
│    vs VolumeMetrix: 0.5×0.3 + 0.2×0.2 + ... = 0.5   │
│    vs ML:           0.5×0.4 + 0.2×0.1 + ... = 0.4   │
└──────────────────────────────────────────────────────┘

Step 3: Apply Softmax (normalize to probabilities)
┌──────────────────────────────────────────────────────┐
│  Raw scores:    [0.8, 0.3, 0.5, 0.4]                │
│  After softmax: [0.40, 0.15, 0.25, 0.20]            │
│                                                      │
│  Interpretation:                                     │
│  - 40% weight to TrendVIC (itself)                  │
│  - 15% weight to Oscillator                          │
│  - 25% weight to VolumeMetrix                        │
│  - 20% weight to ML                                  │
└──────────────────────────────────────────────────────┘

Step 4: Weighted Sum of Values
┌──────────────────────────────────────────────────────┐
│  Output = 0.40×V_TrendVIC + 0.15×V_Osc +            │
│           0.25×V_VolM + 0.20×V_ML                   │
│                                                      │
│  This becomes TrendVIC's new representation,         │
│  enriched with information from other models         │
└──────────────────────────────────────────────────────┘


USAGE EXAMPLE
-------------

from titan.core.fusion import TransformerFusion

# Initialize
fusion = TransformerFusion(config, n_models=4)

# Forward pass
outputs = [trendvic_out, osc_out, volm_out, ml_out]
prob_up, prob_down = fusion.forward(outputs, features)

# After knowing actual outcome
fusion.update(outputs, features, actual_direction="UP")

# Monitor
stats = fusion.get_training_stats()
print(f"Train loss: {stats['train_loss']}")
print(f"Val loss: {stats['val_loss']}")
print(f"Overfitting: {stats['is_overfitting']}")

# Attention analysis
weights = fusion.get_attention_weights()
print(f"Model importance: {weights}")


CONFIGURATION QUICK REFERENCE
------------------------------

Parameter                    Default    Purpose
────────────────────────────────────────────────────────────────────
fusion.enabled               True       Enable/disable fusion
fusion.hidden_dim            32         Model capacity (16-128)
fusion.num_heads             2          Attention heads (1-8)
fusion.dropout               0.2        Regularization (0.0-0.5)
fusion.learning_rate         0.001      Training speed
fusion.l2_lambda             0.01       Weight decay strength
fusion.warmup_steps          100        LR warmup duration
fusion.min_samples           200        Min data before training
fusion.val_split             0.2        Validation ratio
fusion.gradient_clip         1.0        Max gradient norm
fusion.early_stopping_patience  50      Steps before stopping
fusion.early_stopping_delta  0.001      Min improvement


PERFORMANCE CHARACTERISTICS
----------------------------

┌─────────────────────────────────────────────────────────────┐
│  COMPUTATIONAL COST                                         │
│  ───────────────────────────────────────────────────────    │
│  Forward pass:     ~1 ms    (10x simple average)           │
│  Training step:    ~10 ms   (per mini-batch)               │
│  Memory usage:     ~600 KB  (model + buffers)              │
│                                                             │
│  For 1-minute intervals: 60 seconds between predictions    │
│  → Computational cost is negligible                         │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  EXPECTED IMPROVEMENT                                       │
│  ───────────────────────────────────────────────────────    │
│  Accuracy:   +1-3%  (52% → 53-55%)                         │
│  ECE:        Similar or better (1.5-2.0%)                  │
│  Sharpe:     +0.15-0.45  (2.55 → 2.7-3.0)                  │
│                                                             │
│  Mechanism: Non-linear combinations + learned weights      │
│             capture model synergies better than            │
│             simple weighted average                         │
└─────────────────────────────────────────────────────────────┘


FILES CREATED
-------------

titan/core/fusion.py                  695 lines   Main implementation
titan/core/config.py                  +12 params  Configuration
test_fusion.py                        342 lines   Test suite
docs/TRANSFORMER_FUSION.md            400+ lines  Technical docs
docs/FUSION_INTEGRATION_GUIDE.md      300+ lines  Integration guide
docs/FUSION_ARCHITECTURE.txt          This file   Visual reference
FUSION_SUMMARY.md                     500+ lines  Complete summary


================================================================================
End of Architecture Documentation
================================================================================
